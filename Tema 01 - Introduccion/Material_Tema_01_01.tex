\input{../Preambulos/preambulo_materiales}
\usepackage{chemformula}
\title{Física Estadística\vspace{-3ex}}
\author{M. en C. Gustavo Contreras Mayén}
\date{ }

\begin{document}

\vspace{-4cm}
\maketitle
\fontsize{14}{14}\selectfont
\tableofcontents
\newpage

\section{Física Estadística.}
\subsection{Referencia histórica.}

%Ref. Pathria.

La mecánica estadística es un formalismo que tiene como objetivo explicar las propiedades físicas de la materia \enquote{a granel} sobre la base del comportamiento dinámico de sus constituyentes microscópicos. El alcance del formalismo es casi tan ilimitado como el alcance mismo de los fenómenos naturales, pues en principio es aplicable a la materia en cualquier estado que sea. De hecho, se ha aplicado con considerable éxito al estudio de la materia en estado sólido, líquido o gaseoso, materia compuesta de varias fases y/o varios componentes, materia en condiciones extremas de densidad y temperatura, materia en equilibrio con la radiación (como, por ejemplo, en astrofísica), materia en forma de muestra biológica, etc. Además, el formalismo de la mecánica estadística nos permite investigar los \emph{estados de no equilibrio} de la materia tanto como los estados de equilibrio; de hecho, estas investigaciones nos ayudan a comprender la manera en que un sistema físico que está \enquote{fuera de equilibrio} en un momento dado $t$ se acerca a un \enquote{estado de equilibrio} a medida que pasa el tiempo.
\par
En contraste con el estado actual de su desarrollo, el éxito de sus aplicaciones y la amplitud de su alcance, los comienzos de la mecánica estadística fueron bastante modestos. Salvo ciertas referencias primitivas, como las de Gassendi, Hooke, etc., el verdadero trabajo sobre este tema comenzó con las contemplaciones de Bernoulli (1738), Herapath (1821) y Joule (1851), quienes, a su manera individual, intentaron sentar las bases para la llamada teoría cinética de los gases, una disciplina que finalmente resultó ser el precursor de la mecánica estadística. El trabajo pionero de estos investigadores estableció el hecho de que la presión de un gas surgía del movimiento de sus moléculas y, por tanto, podía calcularse considerando la influencia dinámica del bombardeo molecular sobre las paredes del recipiente. Así, Bernoulli y Herapath pudieron demostrar que, si la temperatura permanecía constante, la presión $P$ de un gas ordinario era inversamente proporcional al volumen $V$ del recipiente (ley de Boyle), y que era esencialmente independiente de la forma del recipiente. Esto, por supuesto, implicaba la suposición explícita de que, a \emph{una temperatura $T$ dada}, la velocidad (media) de las moléculas era independiente tanto de la presión como del volumen. Bernoulli incluso intentó determinar la corrección (de primer orden) de esta ley, que surge del tamaño finito de las moléculas, y demostró que el volumen $V$ que aparece en el enunciado de la ley debe reemplazarse por $(V - b)$, donde $b$ es el volumen \enquote{real} de las moléculas.
\par
Joule fue el primero en demostrar que la presión $P$ era directamente proporcional al cuadrado de la velocidad molecular $c$, que inicialmente había supuesto que era la misma para todas las moléculas. Krönig (1856) fue un paso más allá. Introduciendo la suposición \enquote{cuasestadística} de que, en \emph{cualquier momento $t$}, se podría suponer que una sexta parte de las moléculas están volando en cada una de las seis direcciones \enquote{independientes}, llamadas $+x$, $-x$, $+y$, $-y$, $+z$ y $-z$, obtuvo la ecuación:
\begin{align}
P = \dfrac{1}{3} \, n \, m \, c^{2}
\label{eq:ecuacion_01}
\end{align}
donde $n$ es la densidad numérica de las moléculas y $m$ la masa molecular. Krönig también supuso que la velocidad molecular $c$ era la misma para todas las moléculas; así que de la ec. (\ref{eq:ecuacion_01}), infirió que la energía cinética de las moléculas debería ser directamente proporcional a la temperatura absoluta del gas.
\par
Krönig justificó su método con estas palabras: \enquote{La trayectoria de cada molécula debe ser tan irregular que desafíe todos los intentos de cálculo. Sin embargo, de acuerdo con las leyes de probabilidad, ¡uno podría suponer un movimiento completamente regular en lugar de uno completamente irregular!}. Sin embargo, debe notarse que es solo debido a la forma especial de las sumas que aparecen en el cálculo de la presión que el argumento de Krönig lleva al mismo resultado que el que sigue de modelos más refinados. En otros problemas, como los que involucran difusión, viscosidad o conducción de calor, este ya no es el caso.
\par
Fue en este momento cuando Clausius entró en el campo. En primer lugar, en 1857, derivó la ley de los gases ideales bajo supuestos mucho menos estrictos que los de Krönig. Descartó las dos suposiciones principales de Krönig y demostró que la ec. (\ref{eq:ecuacion_01}) seguía siendo cierta; por supuesto, $c^{2}$ ahora se convirtió en la velocidad cuadrática media de las moléculas. En un artículo posterior (1859), Clausius introdujo el concepto de camino libre medio y así se convirtió en el primero en analizar los fenómenos de transporte. Fue en estos estudios donde introdujo la famosa \enquote{Stosszahlansatz} - la hipótesis sobre el número de colisiones (entre las moléculas)- que, más tarde, desempeñó un papel destacado en la obra monumental de Boltzmann. Con Clausius, la introducción de los puntos de vista microscópico y estadístico en la teoría física fue definitivo, más que especulativo. En consecuencia, Maxwell, en un artículo popular titulado \enquote{Moléculas}, escrito para la Enciclopedia Británica, se refirió a Clausius como el \enquote{principal fundador de la teoría cinética de los gases}, mientras que Gibbs, en su nota necrológica de Clausius, lo llamó el \enquote{padre de mecánica estadística}.
\par
El trabajo de Clausius atrajo a Maxwell al campo. Hizo su primera aparición con el libro de memorias \enquote{Ilustraciones en la teoría dinámica de los gases”}(1860), en el que fue mucho más lejos que sus predecesores al derivar su famosa ley de la \enquote{distribución de velocidades moleculares}. La derivación de Maxwell se basó en principios elementales de probabilidad y estaba claramente inspirada en la ley gaussiana de \enquote{distribución de errores aleatorios}. En 1867 apareció una derivación basada en el requisito de que \enquote{la distribución de equilibrio de velocidades moleculares, una vez adquirida, debe permanecer invariable bajo colisiones moleculares}. Esto llevó a Maxwell a establecer lo que se conoce como la ecuación de transporte de Maxwell, que, si se usa hábilmente, conduce a los mismos resultados que uno obtiene de la ecuación más fundamental debido a Boltzmann.
\par
Las contribuciones de Maxwell al tema disminuyeron considerablemente después de su nombramiento, en 1871, como profesor Cavendish en Cambridge. Para entonces, Boltzmann ya había dado sus primeros pasos. En Boltzmann (1868, 1871) generalizó la ley de distribución de Maxwell a los gases poliatómicos, teniendo en cuenta también la presencia de fuerzas externas, si las hubiera; esto dio lugar al famoso \emph{factor de Boltzmann} $\exp(- \beta \varepsilon)$, donde $\varepsilon$ denota la energía total de una molécula. Estas investigaciones también condujeron al teorema de equipartición. Boltzmann demostró además que, al igual que la distribución original de Maxwell, la distribución generalizada (que ahora llamamos distribución de Maxwell-Boltzmann) es estacionaria con respecto a las colisiones moleculares.
\par
En 1872 apareció el célebre \emph{teorema- H}, que proporcionó una base molecular para la tendencia natural de los sistemas físicos a acercarse y permanecer en un estado de equilibrio. Esto estableció una conexión entre el enfoque microscópico (que caracteriza la mecánica estadística) y el enfoque fenomenológico (que caracteriza la termodinámica) mucho más transparente que nunca; también proporcionó un método directo para calcular la entropía de un sistema físico dado a partir de consideraciones puramente microscópicas. Como corolario del teorema-H, Boltzmann demostró que la distribución de Maxwell-Boltzmann es la única distribución que permanece invariable bajo colisiones moleculares y que cualquier otra distribución, bajo la influencia de colisiones moleculares, finalmente pasará a una distribución de Maxwell-Boltzmann. En 1876, Boltzmann derivó su famosa ecuación de transporte, la cual, en manos de Chapman y Enskog (Chapman (1916, 1917); Enskog (1917)), demostró ser una extremadamente poderosa herramienta para investigar las propiedades macroscópicas de los sistemas en estados de no equilibrio. Las cosas, sin embargo, resultaron bastante duras para Boltzmann. Su teorema-H y el consiguiente comportamiento irreversible de los sistemas físicos fueron fuertemente atacados, principalmente por Loschmidt (1876, 1877) y Zermelo (1896). Mientras Loschmidt se preguntaba cómo se podrían reconciliar las consecuencias de este teorema con el carácter reversible de las ecuaciones básicas del movimiento de las moléculas, Zermelo se preguntaba cómo se podían hacer que estas consecuencias encajaran con el comportamiento cuasiperiódico de los sistemas cerrados (que surgió en vista de la llamados ciclos de Poincaré). Boltzmann se defendió contra estos ataques con todas sus fuerzas pero, desafortunadamente, no pudo convencer a sus oponentes de la corrección de su punto de vista. Al mismo tiempo, los energistas, encabezados por Mach y Ostwald, criticaban la base misma (molecular) de la teoría cinética, mientras que Kelvin enfatizaba las \enquote{nubes del siglo XIX que se ciernen sobre la teoría dinámica de la luz y el calor}.
\par
Todo esto dejó a Boltzmann en un estado de desesperación e indujo en él un complejo de persecución. Escribió en la introducción al segundo volumen de su tratado \emph{Vorlesungen über Gastheorie} (1898):
\begin{quote}
Estoy convencido de que los ataques (a la teoría cinética) se basan en malentendidos y que el papel de la teoría cinética aún no se ha jugado. En mi opinión, sería un golpe para la ciencia si la oposición contemporánea hiciera que la teoría cinética se hundiera en el olvido que fue el destino que sufrió la teoría ondulatoria de la luz a través de la autoridad de Newton. Soy consciente de la debilidad de un individuo frente a las corrientes de opinión predominantes. Para asegurar que no habrá que redescubrir demasiado cuando la gente vuelva al estudio de la teoría cinética, presentaré las partes más difíciles y mal entendidas del tema de la manera más clara posible.
\end{quote}

No nos detendremos más en la teoría cinética; preferiríamos continuar con el desarrollo del enfoque más sofisticado conocido como \emph{teoría de conjuntos}, que de hecho puede considerarse como la mecánica estadística propiamente dicha. En este enfoque, el estado dinámico de un sistema dado, caracterizado por las coordenadas generalizadas $q_{i}$ y la cantidad de movimiento generalizada $p_{i}$ está representada por un \emph{punto de fase} $G (q_{i}, p_{i})$ en un \emph{espacio de fase} de dimensionalidad apropiada. La evolución del estado dinámico en el tiempo está representada por la \emph{trayectoria} del punto $G$ en el espacio de fase, la \enquote{geometría} de la trayectoria está gobernada por las ecuaciones de movimiento del sistema y por la naturaleza de las restricciones físicas impuestas en él. Para desarrollar un formalismo apropiado, uno considera el sistema dado junto con un número infinitamente grande de \enquote{copias mentales} del mismo; es decir, un \emph{conjunto (ensamble en inglés)} de sistemas similares bajo restricciones físicas idénticas (aunque, en cualquier momento $t$, los diversos sistemas del conjunto diferirían ampliamente con respecto a sus estados dinámicos). Entonces, en el espacio de fase, uno tiene un enjambre de infinitos puntos $G$ (que, en cualquier momento $t$, están muy dispersos y, con el tiempo, se mueven a lo largo de sus respectivas trayectorias). La ficción de una multitud de sistemas infinitos, idénticos pero independientes, permite reemplazar ciertos supuestos dudosos de la teoría cinética de los gases por afirmaciones fácilmente aceptables de la mecánica estadística. La formulación explícita de estos enunciados fue dada por primera vez por Maxwell (1879), quien en esta ocasión usó la palabra \enquote{estadístico mecánico} para describir el estudio de conjuntos (de sistemas gaseosos) - aunque, 8 años antes, Boltzmann (1871) ya había trabajado esencialmente con el mismo tipo de conjuntos.
\par
La cantidad más importante en la teoría de conjuntos es la \emph{función de densidad}, $\rho (q_{i}, p_{i} ; t)$, de los puntos $G$ en el espacio de fase; una distribución estacionaria $(\pdv*{\rho}{t} = 0)$ caracteriza a un \emph{conjunto estacionario}, que a su vez representa un sistema \emph{en equilibrio}. Maxwell y Boltzmann limitaron su estudio a conjuntos para los que la función $\rho$ dependía únicamente de la energía $E$ del sistema. Esto incluía el caso especial de los \emph{sistemas ergódicos}, que se definían de tal manera que \enquote{el movimiento imperturbable de tal sistema, si se prosiguiera durante un tiempo ilimitado, finalmente atravesaría (la vecindad de) todos y cada uno de los puntos de fase compatibles con el \emph{valor fijo} $E$ de la energía}. En consecuencia, el \emph{promedio del conjunto}, $\expval{f}$, de una cantidad física $f$, tomada en cualquier momento dado $t$, sería el mismo que el \emph{promedio de largo plazo}, $\bar{f}$, perteneciente a cualquier miembro dado del conjunto. Ahora, $\bar{f}$ es el valor que esperamos obtener para la cantidad en cuestión cuando hacemos una medición adecuada en el sistema; el resultado de esta medida debe, por lo tanto, estar de acuerdo con la estimación teórica $\expval{f}$. Adquirimos así una receta para lograr un contacto directo entre teoría y experimento. ¡Al mismo tiempo, establecemos una base racional para una teoría microscópica de la materia como alternativa al enfoque empírico de la termodinámica!
\par
Gibbs hizo un avance significativo en esta dirección, quien, con sus \emph{Principios elementales de mecánica estadística} (1902), convirtió la teoría de conjuntos en una herramienta muy eficiente para el teórico. Hizo hincapié en el uso de conjuntos \enquote{generalizados} y desarrolló esquemas que, en principio, permitían calcular un conjunto completo de cantidades termodinámicas de un sistema dado a partir de propiedades puramente mecánicas de sus constituyentes microscópicos. En sus métodos y resultados, el trabajo de Gibbs resultó ser mucho más general que cualquier tratamiento anterior del tema; se aplicaba a cualquier sistema físico que cumpliera los simples requisitos de que:
\begin{enumerate}[label=\roman*)]
\item tuviera una estructura mecánica.
\item obedeciera las ecuaciones de movimiento de Lagrange y Hamilton.
\end{enumerate}
A este respecto, se puede considerar que el trabajo de Gibbs logró para la termodinámica tanto como lo había logrado Maxwell para la electrodinámica.
\par
Estos desarrollos casi coincidieron con la gran revolución que el trabajo de Planck de 1900 trajo a la física. Como es bien sabido, la \emph{hipótesis cuántica} de Planck resolvió con éxito los misterios esenciales de la radiación del cuerpo negro, un tema en el que se centraron las tres disciplinas mejor establecidas del siglo XIX, a saber, la mecánica, la electrodinámica y la termodinámica. Al mismo tiempo, descubrió tanto las fortalezas como las debilidades de estas disciplinas. Hubiera sido sorprendente que la mecánica estadística, que vinculaba la termodinámica con la mecánica, hubiera escapado a las repercusiones de esta revolución.
\par
El trabajo posterior de Einstein sobre el efecto fotoeléctrico y de Compton sobre la dispersión de rayos X estableció, por así decirlo, la \enquote{existencia} del \emph{cuanto de radiación}, o el \emph{fotón} como ahora llamamos. Entonces era natural que alguien derivara la fórmula de radiación de Planck tratando la radiación de cuerpo negro como un \emph{gas de fotones} de la misma manera que Maxwell había derivado su ley de distribución de velocidades moleculares para un gas de moléculas convencionales. Pero, entonces, ¿un gas de fotones difiere tan radicalmente de un gas de moléculas convencionales que las dos leyes de distribución deberían ser tan diferentes entre sí?
\par
La respuesta a esta pregunta fue proporcionada por la forma en que Bose derivó la fórmula de Planck. En su artículo histórico de 1924, Bose trató la radiación de cuerpo negro como un gas de fotones; sin embargo, en lugar de considerar la asignación de los fotones \enquote{individuales} a los diversos estados de energía del sistema, fijó su atención en el número de estados que contenían \enquote{un número particular} de fotones. Einstein, quien parece haber traducido el artículo de Bose al alemán a partir de un manuscrito en inglés que le envió el autor, reconoció de inmediato la importancia de este enfoque y agregó la siguiente nota a su traducción: \enquote{La derivación de Bose de la fórmula de Planck está en mi opinión un importante paso adelante. El método empleado aquí también produciría la teoría cuántica de un gas ideal, que me propongo demostrar en otro lugar}.
\par
Implícito en el enfoque de Bose estaba el hecho de que, en el caso de los fotones, lo que realmente importaba era \enquote{el conjunto de números de fotones en varios estados de energía del sistema} y no la especificación de \enquote{qué fotón estaba en qué estado}; en otras palabras, los fotones eran mutuamente indistinguibles. Einstein argumentó que lo que Bose había insinuado para los fotones también debería ser cierto para las partículas materiales (porque la propiedad de indistinguibilidad surgía esencialmente del carácter ondulatorio de estas entidades y, según de Broglie, las partículas materiales también poseían ese carácter). En dos artículos, que aparecieron poco después, Einstein (1924, 1925) aplicó el método de Bose al estudio de un gas ideal y, por lo tanto, desarrolló lo que ahora llamamos \emph{estadística de Bose-Einstein}. En el segundo de estos artículos, la diferencia fundamental entre las nuevas estadísticas y las estadísticas clásicas de \emph{Maxwell-Boltzmann} se manifiesta de manera tan transparente en términos de la indistinguibilidad de las moléculas. En el mismo artículo, Einstein descubrió el fenómeno de la \emph{condensación de Bose-Einstein}, que, 13 años después, fue adoptada por London como base para una comprensión microscópica de las curiosas propiedades del $\ch{^4 He}$ líquido a bajas temperaturas.
\par
Siguiendo la enunciación del principio de exclusión de Pauli (1925), Fermi (1926) mostró que ciertos sistemas físicos obedecerían a un tipo diferente de estadísticas, a saber, las \emph{estadísticas de Fermi-Dirac}, en las que no más de una partícula podría ocupar el mismo estado de energía $(n_{i} = 0, 1)$. Parece importante mencionar aquí que el método de Bose de 1924 también conduce a la distribución de Fermi-Dirac, siempre que se limite la ocupación de un estado de energía a una partícula como máximo.
\par
Poco después de su aparición, Fowler (1926) aplicó las estadísticas de Fermi-Dirac para discutir los estados de equilibrio de las estrellas enanas blancas y Pauli (1927) para explicar el paramagnetismo débil e independiente de la temperatura de los metales alcalinos; en cada caso, uno tenía que lidiar con un gas de electrones \enquote{altamente degenerados} que obedecen a las estadísticas de Fermi-Dirac. A raíz de esto, Sommerfeld produjo su obra monumental de 1928 que no solo puso la teoría electrónica de los metales sobre una base físicamente segura, sino que también le dio un nuevo comienzo en la dirección correcta. Así, Sommerfeld pudo explicar prácticamente todas las propiedades principales de los metales que surgieron de los electrones de conducción y, en cada caso, obtuvo resultados que mostraban una concordancia mucho mejor con el experimento que los que se derivaban de las teorías clásicas de Riecke (1898, 1900), Drude (1900) y Lorentz (1904-1905). Casi al mismo tiempo, Thomas (1927) y Fermi (1928) investigaron la distribución de electrones en átomos más pesados y obtuvieron estimaciones teóricas de las energías de enlace relevantes; estas investigaciones condujeron al desarrollo del llamado \emph{modelo del átomo de Thomas-Fermi}, que luego se amplió para que pudiera aplicarse también a moléculas, sólidos y núcleos.
\par
Así, toda la estructura de la mecánica estadística fue revisada por la introducción del concepto de indistinguibilidad de partículas (idénticas). El aspecto estadístico del problema, que ya estaba allí en vista de la gran cantidad de partículas presentes, ahora se vio aumentado por otro aspecto estadístico que surgió de la naturaleza probabilística de la descripción ondulatoria-mecánica. Uno tenía, por lo tanto, que llevar a cabo un \emph{doble promedio} de las variables dinámicas sobre los estados del sistema dado para obtener los valores esperados relevantes. Ese tipo de situación requería una reformulación de la propia teoría del conjunto, que se llevó a cabo paso a paso. Primero, Landau (1927) y von Neumann (1927) introdujeron la llamada \emph{matriz de densidad}, que era el análogo mecánico-cuántico de la función de densidad del espacio de fase clásico; esto fue elaborado, tanto desde el punto de vista estadístico como de la mecánica cuántica, por Dirac (1929-1931). Guiados por la teoría del conjunto clásico, estos autores consideraron conjuntos tanto \emph{microcanónicos} como \emph{canónicos}; Pauli (1927) hizo la introducción de \emph{grandes conjuntos canónicos} en estadística cuántica.
\par
La cuestión importante de qué partículas obedecerían a las estadísticas de Bose-Einstein y cuáles de Fermi-Dirac permaneció teóricamente sin resolver hasta que Belinfante (1939), Pauli (1940) y Belinfante y Pauli (1940) descubrieron la conexión vital entre el espín y la estadística. Resulta que aquellas partículas cuyo spin es un múltiplo entero de $\hbar$ obedecen a la estadística de Bose-Einstein, mientras que aquellas cuyo spin es un múltiplo entero medio impar de $\hbar$ obedecen a la estadística de Fermi-Dirac. Sin embargo, los sistemas bidimensionales permiten estadísticas fraccionarias, en las que el intercambio de dos partículas idénticas introduce un devanado topológico y un factor de fase complejo $e^{i \phi}$, en lugar del convencional $\pm 1$.
\par
Aparte de los hitos anteriores, se han hecho varias contribuciones notables hacia el desarrollo de la mecánica estadística de vez en cuando; sin embargo, la mayoría de esas contribuciones se referían al desarrollo o perfeccionamiento de técnicas matemáticas que hacen más fructífera la aplicación del formalismo básico a problemas físicos reales.

\end{document}